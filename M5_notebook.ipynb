{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess, time, random, gc\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm.callback import early_stopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_grid_name(grid):\n",
    "    name =[x for x in globals() if globals()[x] is grid][0]\n",
    "    return name\n",
    "\n",
    "def reduce_mem_usage(grid, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = grid.memory_usage().sum() / 1024**2    \n",
    "    for col in grid.columns:\n",
    "        col_type = grid[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = grid[col].min()\n",
    "            c_max = grid[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    grid[col] = grid[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    grid[col] = grid[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    grid[col] = grid[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    grid[col] = grid[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    grid[col] = grid[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    grid[col] = grid[col].astype(np.float32)\n",
    "                else:\n",
    "                    grid[col] = grid[col].astype(np.float64)    \n",
    "    end_mem = grid.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print(' Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return grid\n",
    "\n",
    "def submit_to_kaggle(competition_name, submission_file, message):\n",
    "    kaggle_path = \"/root/miniconda3/envs/lightgbm/bin/kaggle\"\n",
    "    subprocess.run([kaggle_path, \"competitions\", \"submit\", \"-c\", competition_name, \"-f\", submission_file, \"-m\", message])\n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'tweedie',\n",
    "    'tweedie_variance_power': 1.1,\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.5,\n",
    "    'device_type': 'cpu',\n",
    "    'subsample_freq': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 2 ** 11 - 1,\n",
    "    'min_data_in_leaf': 2 ** 12 - 1,\n",
    "    'feature_fraction': 0.5,\n",
    "    'max_bin': 100,\n",
    "    'n_estimators': 1400, #1400\n",
    "    'boost_from_average': False,\n",
    "    'verbosity': -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mem. usage decreased to 96.13 Mb (15.9% reduction)\n",
      "479059\n",
      " Mem. usage decreased to 917.98 Mb (47.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "d_cols = [f'd_{i}' for i in range(1, 1942)]\n",
    "d_types = {col: np.int16 for col in d_cols}\n",
    "grid_dtypes = {\n",
    "    'id': 'str',\n",
    "    'item_id': 'str',\n",
    "    'dept_id': 'str',\n",
    "    'cat_id': 'str',\n",
    "    'store_id': 'str',\n",
    "    'state_id': 'str'\n",
    "}\n",
    "grid_dtypes.update(d_types)\n",
    "calendar = pd.read_csv('data/calendar.csv')\n",
    "calendar[['snap_CA','snap_TX','snap_WI']] = calendar[['snap_CA','snap_TX','snap_WI']].astype(bool)\n",
    "sell_prices = pd.read_csv('data/sell_prices.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "grid = pd.read_csv('data/sales_train_evaluation.csv', dtype=grid_dtypes)\n",
    "grid = reduce_mem_usage(grid)\n",
    "\n",
    "##### REMOVE RANDOM ID ONCE HAPPY ########\n",
    "\"\"\" rnd_id = (random.sample(list(grid['id'].unique()), 1)[0]) # FOODS_1_058_WI_2_evaluation\n",
    "grid = grid[grid['id'] == rnd_id] \"\"\"\n",
    "\n",
    "sub_cols = ['id'] + [f'd_{i}' for i in range(1942, 1970)]\n",
    "submission.columns = sub_cols\n",
    "training_days = [f'd_{i}' for i in range(1200, 1942)]\n",
    "req_cols = grid.columns[:6]\n",
    "cols_to_keep = req_cols.tolist() + training_days\n",
    "\n",
    "grid = grid[cols_to_keep]\n",
    "grid = grid.join(submission.set_index('id'), on='id')\n",
    "grid = grid.melt(id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                var_name='d', value_name='sales')\n",
    "grid = grid.merge(calendar.drop(['weekday','year','wday','month'], axis=1), on = ['d'], how = 'left')\n",
    "grid = grid.join(sell_prices.set_index(['store_id','item_id','wm_yr_wk']), on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "grid['sell_price'] = grid['sell_price'].astype(np.float32)\n",
    "print(grid['sell_price'].isna().sum())\n",
    "grid['sell_price'] = grid.groupby(['id'])['sell_price'].ffill()\n",
    "grid['sell_price'] = grid['sell_price'].fillna(-1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "cat_vars = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n",
    "            'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "\n",
    "for var in cat_vars:\n",
    "    grid[var] = le.fit_transform(grid[var])\n",
    "    grid[var] = grid[var].astype('int16')\n",
    "\n",
    "grid['d'] = grid['d'].str.replace('d_', '').astype('int16')    \n",
    "\n",
    "#print(grid[grid['id'] == 'HOUSEHOLD_2_467_WI_3_evaluation'][['id', 'd', 'sales']].sort_values(by=['d']).tail(50))\n",
    "grid['date'] = grid['date'].astype('datetime64[ns]')\n",
    "grid['tm_d'] = grid['date'].dt.day.astype(np.int8)\n",
    "grid['tm_w'] = grid['date'].dt.weekday.astype(np.int8)\n",
    "grid['tm_m'] = grid['date'].dt.month.astype(np.int8)\n",
    "grid['tm_y'] = grid['date'].dt.year\n",
    "grid['tm_y'] = (grid['tm_y'] - grid['tm_y'].min()).astype(np.int8)\n",
    "grid['tm_dw'] = grid['date'].dt.dayofweek.astype(np.int8)\n",
    "grid['tm_w_end'] = (grid['tm_dw'] >= 5).astype(np.int8)\n",
    "grid = grid.drop(['wm_yr_wk', 'date'], axis=1)\n",
    "grid = grid.sort_values(by=['id','d'], ascending=[True,True])\n",
    "grid = reduce_mem_usage(grid)\n",
    "grid.to_pickle('data/grid_no_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mem. usage decreased to 34.60 Mb (80.2% reduction)\n",
      "Train rows: 30490 22776030\n",
      "492042\n",
      " Mem. usage decreased to 743.66 Mb (74.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "eva = pd.read_csv('data/sales_train_evaluation.csv')\n",
    "eva = eva.iloc[:,np.r_[0:6, 1200:len(eva.columns)]] #\n",
    "eva = reduce_mem_usage(eva)\n",
    "price = pd.read_csv('data/sell_prices.csv')\n",
    "calendar = pd.read_csv('data/calendar.csv')\n",
    "\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1913+28      # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns\n",
    "\n",
    "grid = pd.melt(eva, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "print('Train rows:', len(eva), len(grid))\n",
    "\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = eva[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "del eva\n",
    "grid = pd.concat([grid,add_grid])\n",
    "del add_grid\n",
    "grid = grid.reset_index(drop=True)\n",
    "grid = grid.merge(calendar.drop(['weekday','year','wday','month'], axis=1), on = ['d'], how = 'left')\n",
    "grid = grid.merge(price, on = ['store_id','item_id','wm_yr_wk'], how = 'left')\n",
    "print(grid['sell_price'].isna().sum())\n",
    "grid['date'] = grid['date'].astype('datetime64[ns]')\n",
    "grid['tm_d'] = grid['date'].dt.day.astype(np.int8)\n",
    "grid['tm_w'] = grid['date'].dt.weekday.astype(np.int8)\n",
    "grid['tm_m'] = grid['date'].dt.month.astype(np.int8)\n",
    "grid['tm_y'] = grid['date'].dt.year\n",
    "grid['tm_y'] = (grid['tm_y'] - grid['tm_y'].min()).astype(np.int8)\n",
    "grid['tm_dw'] = grid['date'].dt.dayofweek.astype(np.int8)\n",
    "grid['tm_w_end'] = (grid['tm_dw'] >= 5).astype(np.int8)\n",
    "grid['d'] = grid['d'].str.replace('d_', '').astype('int16')\n",
    "grid = grid.drop(['wm_yr_wk', 'date'], axis=1)\n",
    "cat_vars = ['item_id','dept_id','cat_id','store_id','state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "grid[cat_vars] = grid[cat_vars].astype(str)\n",
    "le = LabelEncoder()\n",
    "\n",
    "for cat in cat_vars:\n",
    "    grid[cat] = le.fit_transform(grid[cat])\n",
    "grid = reduce_mem_usage(grid)\n",
    "grid.to_pickle('data/base.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ ROLLING MEANS ************\n",
      "************ ROLLING STATS ************\n",
      "************ DIFF MEANS ************\n",
      "************ LAGS ************\n",
      "************ ROLLING ZEROS ************\n",
      " Mem. usage decreased to 2005.62 Mb (58.6% reduction)\n",
      "************ ROLLING PRICES ************\n",
      "sales                853720\n",
      "sell_price           492042\n",
      "rm_7                1036660\n",
      "rm_14               1250090\n",
      "rm_30               1737930\n",
      "rm_60               2652630\n",
      "rm_180              6311430\n",
      "max_7               1036660\n",
      "std_7               1036660\n",
      "max_14              1250090\n",
      "std_14              1250090\n",
      "max_30              1737930\n",
      "std_30              1737930\n",
      "max_60              2652630\n",
      "std_60              2652630\n",
      "max_180             6311430\n",
      "std_180             6311430\n",
      "diff_rm_7           1067150\n",
      "diff_rm_56          2561160\n",
      "diff_rm_140         5122320\n",
      "lag_0                853720\n",
      "lag_1                853720\n",
      "lag_2                853720\n",
      "lag_3                853720\n",
      "lag_4                853720\n",
      "lag_5                853720\n",
      "lag_6                853720\n",
      "rolling_zero_7       182940\n",
      "rolling_zero_56     1676950\n",
      "rolling_zero_140    4238110\n",
      "price_max            492042\n",
      "price_min            492042\n",
      "price_std            522532\n",
      "price_mean           492042\n",
      "price_norm           492042\n",
      "item_nunique         492042\n",
      "dtype: int64\n",
      " Mem. usage decreased to 2298.58 Mb (30.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "#grid = pd.read_pickle('data/grid_no_features.pkl')\n",
    "grid = pd.read_pickle('data/base.pkl')\n",
    "\n",
    "print('************ ROLLING MEANS ************')\n",
    "grp = grid.groupby(['id'], group_keys=False)['sales']\n",
    "for roll in [7,14,30,60,180]:\n",
    "    grid['rm_' + str(roll)] = grp.apply(lambda x: x.rolling(roll).mean())\n",
    "   \n",
    "print('************ ROLLING STATS ************')\n",
    "for roll in [7,14,30,60,180]:\n",
    "    grid['max_' + str(roll)] = grp.apply(lambda x: x.rolling(roll).max())\n",
    "    grid['std_' + str(roll)] = grp.apply(lambda x: x.rolling(roll).std())\n",
    "\n",
    "print('************ DIFF MEANS ************')\n",
    "for l in [7,56,140]:\n",
    "    grid['diff_rm_' + str(l)] = grp.apply(lambda x : x.diff().rolling(l).mean()) \n",
    "\n",
    "grp = grid.groupby(['id'], group_keys=False)['sales']\n",
    "print('************ LAGS ************')\n",
    "for lag in [0,1,2,3,4,5,6]:\n",
    "    grid['lag_' + str(lag)] = grp.apply(lambda x: x.shift(lag))\n",
    "\n",
    "print('************ ROLLING ZEROS ************')\n",
    "for roll in [7,56,140]:\n",
    "    grid['is_zero'] = [1 if sales == 0 else 0 for sales in grid['sales']]\n",
    "    grp = grid.groupby(['id'], group_keys=False)['is_zero']\n",
    "    grid['rolling_zero_' + str(roll)] = grp.apply(lambda x : x.rolling(roll).sum())\n",
    "    grid = grid.drop('is_zero', axis = 1)   \n",
    "\n",
    "grid = reduce_mem_usage(grid)\n",
    "grid['sell_price'] = grid['sell_price'].astype(np.float32)\n",
    "print('************ ROLLING PRICES ************')\n",
    "# If we remove expanding all row values for a given id become the same\n",
    "grp = grid.groupby(['id'], group_keys=False)['sell_price']\n",
    "grid['price_max'] = grp.apply(lambda x : x.expanding().max())\n",
    "grid['price_min'] = grp.apply(lambda x : x.expanding().min())\n",
    "grid['price_std'] = grp.apply(lambda x : x.expanding().std())\n",
    "grid['price_mean'] = grp.apply(lambda x : x.expanding().mean())\n",
    "grid['price_norm'] = grid['sell_price']/grid['price_max']\n",
    "grid['price_nunique'] = grid.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "grid['item_nunique'] = grid.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "na_cols=grid.isna().sum()\n",
    "print(na_cols[na_cols>0])\n",
    "grid = reduce_mem_usage(grid)\n",
    "#grid.to_pickle('data/grid_features.pkl')\n",
    "grid.to_pickle('data/feats.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23629750, 57)\n",
      "train columns: Index(['item_id', 'dept_id', 'cat_id', 'event_name_1', 'event_type_1',\n",
      "       'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI',\n",
      "       'sell_price', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_dw', 'tm_w_end',\n",
      "       'rm_7', 'rm_14', 'rm_30', 'rm_60', 'rm_180', 'max_7', 'std_7', 'max_14',\n",
      "       'std_14', 'max_30', 'std_30', 'max_60', 'std_60', 'max_180', 'std_180',\n",
      "       'diff_rm_7', 'diff_rm_56', 'diff_rm_140', 'lag_0', 'lag_1', 'lag_2',\n",
      "       'lag_3', 'lag_4', 'lag_5', 'lag_6', 'rolling_zero_7', 'rolling_zero_56',\n",
      "       'rolling_zero_140', 'price_max', 'price_min', 'price_std', 'price_mean',\n",
      "       'price_norm', 'price_nunique', 'item_nunique'],\n",
      "      dtype='object')\n",
      "(23629750, 57)\n",
      "************ Training Store 1 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[355]\tvalid_0's rmse: 1.99779\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[284]\tvalid_0's rmse: 2.19391\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[229]\tvalid_0's rmse: 2.3124\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[209]\tvalid_0's rmse: 2.32911\n",
      "************ Training Store 2 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[468]\tvalid_0's rmse: 1.97812\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[381]\tvalid_0's rmse: 2.04518\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's rmse: 2.01559\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[515]\tvalid_0's rmse: 2.00561\n",
      "************ Training Store 3 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[428]\tvalid_0's rmse: 2.44907\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[402]\tvalid_0's rmse: 2.5779\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[243]\tvalid_0's rmse: 2.5912\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's rmse: 2.78258\n",
      "************ Training Store 4 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's rmse: 1.33415\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[251]\tvalid_0's rmse: 1.37998\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[232]\tvalid_0's rmse: 1.49613\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[203]\tvalid_0's rmse: 1.51238\n",
      "************ Training Store 5 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's rmse: 1.63643\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid_0's rmse: 1.64366\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid_0's rmse: 1.89242\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's rmse: 1.89291\n",
      "************ Training Store 6 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's rmse: 1.75164\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's rmse: 1.95819\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[172]\tvalid_0's rmse: 2.02888\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[214]\tvalid_0's rmse: 1.94158\n",
      "************ Training Store 7 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[252]\tvalid_0's rmse: 1.86276\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[232]\tvalid_0's rmse: 2.07061\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[528]\tvalid_0's rmse: 2.11694\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[556]\tvalid_0's rmse: 2.01047\n",
      "************ Training Store 8 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[406]\tvalid_0's rmse: 1.64792\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalid_0's rmse: 1.66819\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[364]\tvalid_0's rmse: 1.76658\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's rmse: 1.68609\n",
      "************ Training Store 9 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[472]\tvalid_0's rmse: 2.22274\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[470]\tvalid_0's rmse: 3.52119\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[214]\tvalid_0's rmse: 3.6504\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[212]\tvalid_0's rmse: 2.49709\n",
      "************ Training Store 10 ************\n",
      "Val start: 1914. Val end 1920. Test start 1942 Pred end 1948\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[404]\tvalid_0's rmse: 1.75867\n",
      "Val start: 1921. Val end 1927. Test start 1949 Pred end 1955\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[407]\tvalid_0's rmse: 2.27325\n",
      "Val start: 1928. Val end 1934. Test start 1956 Pred end 1962\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalid_0's rmse: 2.10845\n",
      "Val start: 1935. Val end 1941. Test start 1963 Pred end 1969\n",
      "Train shape: (2192231, 52). Val shape: (21343, 52). Test shape: (21343, 52)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[469]\tvalid_0's rmse: 1.87922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 12:59:59,663 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f5f0b8306a0>: Failed to resolve 'www.kaggle.com' ([Errno -3] Temporary failure in name resolution)\")': /api/v1/competitions/m5-forecasting-accuracy/submissions/url/21485593/1697198393\n",
      "2023-10-13 13:00:05,903 WARNING Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f5f0b8323e0>: Failed to resolve 'www.kaggle.com' ([Errno -3] Temporary failure in name resolution)\")': /api/v1/competitions/m5-forecasting-accuracy/submissions/url/21485593/1697198393\n",
      "2023-10-13 13:00:12,143 WARNING Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f5f0b8321d0>: Failed to resolve 'www.kaggle.com' ([Errno -3] Temporary failure in name resolution)\")': /api/v1/competitions/m5-forecasting-accuracy/submissions/url/21485593/1697198393\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connection.py\", line 203, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/util/connection.py\", line 60, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/socket.py\", line 955, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno -3] Temporary failure in name resolution\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 790, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 491, in _make_request\n",
      "    raise new_e\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 467, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1092, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connection.py\", line 611, in connect\n",
      "    self.sock = sock = self._new_conn()\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connection.py\", line 210, in _new_conn\n",
      "    raise NameResolutionError(self.host, self, e) from e\n",
      "urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x7f5f0b832020>: Failed to resolve 'www.kaggle.com' ([Errno -3] Temporary failure in name resolution)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/lightgbm/bin/kaggle\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/cli.py\", line 70, in main\n",
      "    out = args.func(**command_args)\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/api/kaggle_api_extended.py\", line 801, in competition_submit_cli\n",
      "    submit_result = self.competition_submit(file_name, message,\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/api/kaggle_api_extended.py\", line 749, in competition_submit\n",
      "    self.competitions_submissions_url_with_http_info(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/api/kaggle_api.py\", line 1046, in competitions_submissions_url_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/api_client.py\", line 329, in call_api\n",
      "    return self.__call_api(resource_path, method,\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/api_client.py\", line 161, in __call_api\n",
      "    response_data = self.request(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/api_client.py\", line 371, in request\n",
      "    return self.rest_client.POST(url,\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/rest.py\", line 282, in POST\n",
      "    return self.request(\"POST\", url,\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/kaggle/rest.py\", line 194, in request\n",
      "    r = self.pool_manager.request(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/_request_methods.py\", line 118, in request\n",
      "    return self.request_encode_body(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/_request_methods.py\", line 217, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 874, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 874, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 874, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 844, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/root/miniconda3/envs/lightgbm/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.kaggle.com', port=443): Max retries exceeded with url: /api/v1/competitions/m5-forecasting-accuracy/submissions/url/21485593/1697198393 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f5f0b832020>: Failed to resolve 'www.kaggle.com' ([Errno -3] Temporary failure in name resolution)\"))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "      \n",
    "       horizon = 28\n",
    "       grid = pd.read_pickle('data/feats.pkl')\n",
    "       print(grid.shape)\n",
    "       STEPS = [7,14,21,28] # Training/Prediction every 7 days (Compromise between very granular and long time horizon)\n",
    "       TARGET = ['sales']\n",
    "       VAL_DAYS, TEST_DAYS = STEPS[0], STEPS[0]\n",
    "       STORES = grid.store_id.unique()\n",
    "       DEPTS = grid.dept_id.unique()\n",
    "\n",
    "       train_start = grid.d.min()\n",
    "       train_end = 1941 - horizon\n",
    "       first_val_day = train_end + 1\n",
    "       last_val_day = 1941\n",
    "       first_pred_day = 1941 + 1\n",
    "\n",
    "       predictions = pd.DataFrame()\n",
    "       remove_colums = ['id', 'store_id', 'state_id', 'd', 'sales']\n",
    "       train_columns = grid.columns[~grid.columns.isin(remove_colums)]\n",
    "       lags_columns = ['rm_7', 'rm_14', 'rm_30', 'rm_60',\n",
    "       'rm_180', 'max_7', 'std_7', 'max_14', 'std_14', 'max_30', 'std_30',\n",
    "       'max_60', 'std_60', 'max_180', 'std_180', 'diff_rm_7', 'diff_rm_56',\n",
    "       'diff_rm_140', 'lag_0', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5',\n",
    "       'lag_6', 'rolling_zero_7', 'rolling_zero_56', 'rolling_zero_140']\n",
    "\n",
    "       print(f'train columns: {train_columns}')\n",
    "       print(grid.shape)\n",
    "\n",
    "       for store in STORES:\n",
    "              print(f'************ Training Store {store+1} ************')\n",
    "              #for dept in DEPTS:\n",
    "              for step in STEPS:\n",
    "\n",
    "                     grid = pd.read_pickle('data/feats.pkl')\n",
    "                     grid = grid[(grid['store_id'] == store)]# & (grid['dept_id'] == dept)]\n",
    "                     grid[lags_columns] = grid.groupby(['id'], observed=False)[lags_columns].shift(step)\n",
    "                     mask = (grid['d'] <= 1941) & np.isnan(grid['sales'])\n",
    "                     grid = grid.loc[~mask]\n",
    "\n",
    "                     val_start = first_val_day + step - VAL_DAYS\n",
    "                     val_end = first_val_day + step - 1\n",
    "                     pred_start = first_pred_day + step - VAL_DAYS \n",
    "                     pred_end = first_pred_day + step - 1\n",
    "                     print(f'Val start: {val_start}. Val end {val_end}. Test start {pred_start} Pred end {pred_end}')\n",
    "\n",
    "                     trainX = grid[(grid['d'] <= train_end)][train_columns]\n",
    "                     trainY = grid[(grid['d'] <= train_end)][TARGET]\n",
    "                     valX = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][train_columns]\n",
    "                     valY = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][TARGET]\n",
    "                     testX = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][train_columns]\n",
    "                     print(f'Train shape: {trainX.shape}. Val shape: {valX.shape}. Test shape: {testX.shape}')\n",
    "                     # rnd_id = (random.sample(list(grid['id'].unique()), 1)[0])\n",
    "                     # print(grid[(grid['id'] == rnd_id) & (grid['d'] <= 1969)][['id', 'd', 'sales', 'lag_0', 'lag_1', 'lag_3', 'rm_7', 'rm_14']].sort_values(by=['d']).tail(35))\n",
    "\n",
    "                     # Train\n",
    "                     lgbm = LGBMRegressor(**lgb_params)\n",
    "                     callbacks = [early_stopping(stopping_rounds=50, first_metric_only=False)]\n",
    "\n",
    "                     lgbm.fit(trainX, trainY,\n",
    "                            eval_set=[(valX, valY)],\n",
    "                            eval_metric='rmse',\n",
    "                            callbacks=callbacks)\n",
    "\n",
    "                     # Predict\n",
    "                     yhat = lgbm.predict(testX, num_iteration=lgbm.best_iteration_)\n",
    "                     preds = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][['id', 'd']]\n",
    "                     preds['sales'] = yhat\n",
    "                     predictions = pd.concat([predictions, preds], axis=0)\n",
    "\n",
    "                     \"\"\"gain_importances = lgbm.booster_.feature_importance(importance_type='gain')\n",
    "                     sorted_indices = gain_importances.argsort()[::-1]\n",
    "                     for index in sorted_indices:\n",
    "                            print(f\"{trainX.columns[index]}: {gain_importances[index]}\") \"\"\"\n",
    "\n",
    "       # Submission\n",
    "       predictions.to_pickle(f'submissions/store_dpt_4days.pkl')\n",
    "       submission = pd.read_csv('data/sample_submission.csv')\n",
    "       predictions = predictions.pivot(index='id', columns='d', values='sales').reset_index()\n",
    "       predictions.columns = submission.columns\n",
    "       predictions = submission[['id']].merge(predictions, on='id', how='left').fillna(1)\n",
    "       submission_file = \"submissions/submission.csv\"\n",
    "       predictions.to_csv(f'{submission_file}', index=False)\n",
    "       message = \"Automated submission\"\n",
    "       competition_name = \"m5-forecasting-accuracy\"\n",
    "       submit_to_kaggle(competition_name, submission_file, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object\n",
       "item_id           int16\n",
       "dept_id            int8\n",
       "cat_id             int8\n",
       "store_id           int8\n",
       "state_id           int8\n",
       "d                 int16\n",
       "sales             int16\n",
       "event_name_1       int8\n",
       "event_type_1       int8\n",
       "event_name_2       int8\n",
       "event_type_2       int8\n",
       "snap_CA            int8\n",
       "snap_TX            int8\n",
       "snap_WI            int8\n",
       "sell_price      float16\n",
       "tm_d               int8\n",
       "tm_w               int8\n",
       "tm_m               int8\n",
       "tm_y               int8\n",
       "tm_dw              int8\n",
       "tm_w_end           int8\n",
       "rm_7            float16\n",
       "rm_14           float16\n",
       "rm_30           float16\n",
       "rm_60           float16\n",
       "rm_180          float16\n",
       "lag_0             int16\n",
       "lag_1           float16\n",
       "lag_2           float16\n",
       "lag_3           float16\n",
       "lag_4           float16\n",
       "lag_5           float16\n",
       "lag_6           float16\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvw_grid = pd.read_pickle('data/grid_features.pkl')\n",
    "rvw_cols = ['id','d','sales','snap_CA', 'snap_TX', 'snap_WI','lag_0','lag_1','lag_2','rm_7','rm_14']\n",
    "rvw_grid[rvw_grid['id']=='FOODS_1_002_WI_3_evaluation'][rvw_cols].tail(28+14)\n",
    "rvw_grid.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try\n",
    "1. Try to use -1 for NAs\n",
    "2. Do nothing with NAs\n",
    "3. Leave lags+rms and try more steps --> 2 steps > 1 step > 7 steps (4steps missing)\n",
    "4. Leave lags+rms and try store+dept --> almost negligible improvement (bug still exists)\n",
    "5. Get more data --> 0.2 improvement (bug still there)\n",
    "6. Remove categorical variables ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60c112127e11c5371fc1371160fc4d27b7aede14e0d229d3bdb847bc9c3e674c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
