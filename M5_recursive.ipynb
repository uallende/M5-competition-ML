{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import subprocess, psutil, os\n",
    "\n",
    "from lightgbm.callback import early_stopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_grid_name(grid):\n",
    "    name =[x for x in globals() if globals()[x] is grid][0]\n",
    "    return name\n",
    "\n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def reduce_mem_usage(grid, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = grid.memory_usage().sum() / 1024**2    \n",
    "    for col in grid.columns:\n",
    "        col_type = grid[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = grid[col].min()\n",
    "            c_max = grid[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    grid.loc[:,col] = grid[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    grid.loc[:,col] = grid[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    grid.loc[:,col] = grid[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    grid.loc[:,col] = grid[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    grid.loc[:,col] = grid[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    grid.loc[:,col] = grid[col].astype(np.float32)\n",
    "                else:\n",
    "                    grid.loc[:,col] = grid[col].astype(np.float64)    \n",
    "    end_mem = grid.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print(' Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return grid\n",
    "\n",
    "def submit_to_kaggle(competition_name, submission_file, message):\n",
    "    kaggle_path = \"/root/miniconda3/envs/lightgbm/bin/kaggle\"\n",
    "    subprocess.run([kaggle_path, \"competitions\", \"submit\", \"-c\", competition_name, \"-f\", submission_file, \"-m\", message])\n",
    "\n",
    "## Merging by concat to not lose dtypes\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1    \n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'tweedie',\n",
    "    'tweedie_variance_power': 1.1,\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.5,\n",
    "    'device_type': 'cpu',\n",
    "    'subsample_freq': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 2 ** 11 - 1,\n",
    "    'min_data_in_leaf': 2 ** 12 - 1,\n",
    "    'feature_fraction': 0.5,\n",
    "    'max_bin': 100,\n",
    "    'boost_from_average': False,\n",
    "    'verbosity': -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('recursive/no_feat.pkl'):\n",
    "    print(\"The file 'no_feat.pkl' already exists. Skipping save operation.\")\n",
    "\n",
    "else:    \n",
    "    TARGET = 'sales'         # Our main target\n",
    "    END_TRAIN = 1941         # Last day in train set\n",
    "    MAIN_INDEX = ['id','d']  # We can identify item by these columns\n",
    "\n",
    "    eva = pd.read_csv('data/sales_train_evaluation.csv')\n",
    "    print('Create Grid')\n",
    "    index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "    grid = pd.melt(eva, \n",
    "                    id_vars = index_columns, \n",
    "                    var_name = 'd', \n",
    "                    value_name = TARGET)\n",
    "\n",
    "    print(f'Train rows. Wide: {len(eva)}, Deep: {len(grid)}')\n",
    "\n",
    "    add_grid = pd.DataFrame()\n",
    "    for i in range(1,29):\n",
    "        temp_df = eva[index_columns]\n",
    "        temp_df = temp_df.drop_duplicates()\n",
    "        temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "        temp_df[TARGET] = np.nan\n",
    "        add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "    grid = pd.concat([grid,add_grid])\n",
    "    grid = grid.reset_index(drop=True)\n",
    "\n",
    "    del temp_df, add_grid, eva\n",
    "    print(\"{:>20}: {:>8}\".format('Original grid',sizeof_fmt(grid.memory_usage(index=True).sum())))\n",
    "\n",
    "    for col in index_columns:\n",
    "        grid[col] = grid[col].astype('category')\n",
    "\n",
    "    print(\"{:>20}: {:>8}\".format('Reduced grid',sizeof_fmt(grid.memory_usage(index=True).sum())))\n",
    "    grid = reduce_mem_usage(grid)\n",
    "\n",
    "    price = pd.read_csv('data/sell_prices.csv')\n",
    "    calendar = pd.read_csv('data/calendar.csv')\n",
    "    print('Release week')\n",
    "\n",
    "    release_df = price.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "    release_df.columns = ['store_id','item_id','release']\n",
    "\n",
    "    grid = merge_by_concat(grid, release_df, ['store_id','item_id'])\n",
    "    del release_df\n",
    "\n",
    "    grid = merge_by_concat(grid, calendar[['wm_yr_wk','d']], ['d'])\n",
    "    grid = grid[grid['wm_yr_wk']>=grid['release']]\n",
    "    grid = grid.reset_index(drop=True)\n",
    "    grid = reduce_mem_usage(grid)\n",
    "\n",
    "    grid = merge_by_concat(grid, price, ['store_id','item_id','wm_yr_wk'])\n",
    "    grid = reduce_mem_usage(grid)\n",
    "    print(grid.columns)\n",
    "    del price, calendar\n",
    "    grid['release'] = grid['release'] - grid['release'].min()\n",
    "    grid['release'] = grid['release'].astype(np.int16)\n",
    "\n",
    "    price = pd.read_pickle('data/prices.pkl')\n",
    "    grid = grid.merge(price.drop(['sell_price'], axis=1), on = ['store_id','item_id','wm_yr_wk'], how='left')\n",
    "\n",
    "    calendar = pd.read_csv('data/calendar.csv')\n",
    "    grid = grid.merge(calendar.drop(['weekday','year','wday','month','wm_yr_wk'], axis=1), on = ['d'], how = 'left')\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    cat_vars = ['item_id','store_id','dept_id','cat_id','state_id','event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "    del price, calendar\n",
    "    for cat in cat_vars:\n",
    "        grid[cat] = le.fit_transform(grid[cat])\n",
    "\n",
    "    grid['date'] = grid['date'].astype('datetime64[ns]')\n",
    "    grid['tm_d'] = grid['date'].dt.day.astype(np.int8)\n",
    "    grid['tm_w'] = grid['date'].dt.isocalendar().week.astype(np.int8)\n",
    "    grid['tm_m'] = grid['date'].dt.month.astype(np.int8)\n",
    "    grid['tm_y'] = grid['date'].dt.year\n",
    "    grid['tm_y'] = (grid['tm_y'] - grid['tm_y'].min()).astype(np.int8)\n",
    "    grid['tm_dw'] = grid['date'].dt.dayofweek.astype(np.int8)\n",
    "    grid['tm_w_end'] = (grid['tm_dw'] >= 5).astype(np.int8)\n",
    "    grid['d'] = grid['d'].str.replace('d_', '').astype('int16')\n",
    "    grid = reduce_mem_usage(grid)\n",
    "    grid.to_pickle('recursive/no_feat.pkl')\n",
    "    del grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(grid):\n",
    "    grid = grid[['id','d','sales']].copy()\n",
    "    grp = grid.groupby(['id'], group_keys=False, observed=False)['sales']\n",
    "    \n",
    "    grid = reduce_mem_usage(grid)\n",
    "    print('************ ROLLING LAGS ************')\n",
    "    \n",
    "    for roll in [7, 14, 30, 60]:\n",
    "        grid.loc[:, f'rm_{roll}'] = grp.transform(lambda x: x.rolling(roll).mean())\n",
    "        grid.loc[:, f'diff_rm_{roll}'] = grp.transform(lambda x: x.diff().rolling(roll).mean())\n",
    "        \n",
    "    for s_window in [1, 7, 14]:\n",
    "        for roll in [7, 14, 30, 60]:\n",
    "            grid.loc[:, f'rm_shifted_{roll}'] = grp.transform(lambda x: x.shift(s_window).rolling(roll).mean())\n",
    "            \n",
    "    grid = reduce_mem_usage(grid)\n",
    "    print('************ LAGS ************')\n",
    "    \n",
    "    for lag in np.arange(0, 15, 1):\n",
    "        grid.loc[:, f'lag_{lag}'] = grp.transform(lambda x: x.shift(lag))\n",
    "        \n",
    "    grid = grid.drop(['sales'], axis=1)\n",
    "    ix_to_drop = grid[(grid['d'] <= 1941) & (grid.isna().any(axis=1))].index\n",
    "    grid.drop(index=ix_to_drop, inplace=True)\n",
    "    grid = reduce_mem_usage(grid)\n",
    "    return grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 28\n",
    "grid = pd.read_pickle('recursive/grid.pkl')\n",
    "grid = grid[grid['d']>=1700]\n",
    "\n",
    "TARGET = ['sales']\n",
    "DAYS = [14,28]\n",
    "step = DAYS[0]\n",
    "STORES = grid.store_id.unique()\n",
    "DEPTS = grid.dept_id.unique()\n",
    "LAST_D = 1941\n",
    "\n",
    "remove_colums = ['id','item_id','dept_id','cat_id','store_id','state_id','d','sales','wm_yr_wk','date']\n",
    "lags_columns = feats.columns[len(remove_colums):]\n",
    "train_columns = list(grid.columns[~grid.columns.isin(remove_colums)]) + list(lags_columns) \n",
    "print(train_columns)\n",
    "\n",
    "grid[lags_columns] = grid.groupby(['id'], observed=False)[lags_columns].shift(step)\n",
    "ix_to_drop = grid[(grid['d'] <= 1941) & (grid.isna().any(axis=1))].index\n",
    "grid.drop(index=ix_to_drop, inplace=True)\n",
    "\n",
    "for store in STORES:\n",
    "    grid = pd.read_pickle('recursive/grid.pkl')\n",
    "    grid = grid[grid['store_id']==store]\n",
    "    trainX = grid[grid['d']<=1913][train_columns]\n",
    "    trainY = grid[grid['d']<=1913][TARGET]\n",
    "    valX = grid[(grid['d']>1913) & (grid['d']<=LAST_D)][train_columns]\n",
    "    valY = grid[(grid['d']>1913) & (grid['d']<=LAST_D)][TARGET]\n",
    "    train_set = lgb.Dataset(trainX, label=trainY)\n",
    "    val_set = lgb.Dataset(valX, label=valY)\n",
    "\n",
    "    callbacks = [early_stopping(stopping_rounds=50, first_metric_only=False)]\n",
    "    model = lgb.train(params=lgb_params,\n",
    "                    train_set=train_set,\n",
    "                    valid_sets=val_set,\n",
    "                    num_boost_round=1400,\n",
    "                    callbacks=callbacks)\n",
    "    model.save_model(f'models/lgbm_{store}.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train columns: ['release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'prev_sell_price', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_dw', 'tm_w_end', 'rolling_zero_7', 'rm_7', 'std_7', 'diff_rm_7', 'rolling_zero_14', 'rm_14', 'std_14', 'diff_rm_14', 'rolling_zero_30', 'rm_30', 'std_30', 'diff_rm_30', 'rolling_zero_60', 'rm_60', 'std_60', 'diff_rm_60', 'lag_0', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'lag_8', 'lag_9', 'lag_10', 'lag_11', 'lag_12', 'lag_13', 'lag_14', 'cat_id_enc', 'item_id_enc', 'dept_id_enc', 'store_id_enc', 'store_id_cat_id_enc', 'store_id_item_id_enc', 'store_id_dept_id_enc']\n",
      " Mem. usage decreased to  9.19 Mb (0.0% reduction)\n",
      "************ ROLLING LAGS ************\n",
      " Mem. usage decreased to 81.81 Mb (0.0% reduction)\n",
      "************ LAGS ************\n",
      " Mem. usage decreased to 19.15 Mb (0.0% reduction)\n",
      " Mem. usage decreased to 51.90 Mb (0.0% reduction)\n",
      "************ CATEGORIES ENCODED ************\n",
      " Mem. usage decreased to 60.85 Mb (0.0% reduction)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['rolling_zero_7', 'std_7', 'rolling_zero_14', 'std_14', 'rolling_zero_30', 'std_30', 'rolling_zero_60', 'std_60'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m grid \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mmerge(enc_feats, on\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m], how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mdel\u001b[39;00m enc_feats\n\u001b[0;32m---> 38\u001b[0m grid[lags_columns] \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m], observed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39;49mshift(step)[lags_columns] \n\u001b[1;32m     39\u001b[0m pred_msk \u001b[39m=\u001b[39m ( \u001b[39m#85372 rows for stores\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     (grid[\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m>\u001b[39mlast_day\u001b[39m+\u001b[39mday\u001b[39m-\u001b[39mstep) \u001b[39m&\u001b[39m \n\u001b[1;32m     41\u001b[0m     (grid[\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m<\u001b[39m\u001b[39m=\u001b[39mlast_day\u001b[39m+\u001b[39mday) \u001b[39m&\u001b[39m \n\u001b[1;32m     42\u001b[0m     (grid[\u001b[39m'\u001b[39m\u001b[39mstore_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39mstore)\n\u001b[1;32m     43\u001b[0m ) \n\u001b[1;32m     44\u001b[0m testX \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mloc[pred_msk, train_columns]\n",
      "File \u001b[0;32m~/miniconda3/envs/lightgbm/lib/python3.10/site-packages/pandas/core/frame.py:3902\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3900\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3901\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3902\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3904\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lightgbm/lib/python3.10/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lightgbm/lib/python3.10/site-packages/pandas/core/indexes/base.py:6178\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6175\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 6178\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['rolling_zero_7', 'std_7', 'rolling_zero_14', 'std_14', 'rolling_zero_30', 'std_30', 'rolling_zero_60', 'std_60'] not in index\""
     ]
    }
   ],
   "source": [
    "horizon = 28\n",
    "grid = pd.read_pickle('recursive/grid.pkl')\n",
    "grid = grid[grid['d'] >=1750]\n",
    "\n",
    "TARGET = ['sales']\n",
    "DAYS = [14,28]\n",
    "STORES = grid.store_id.unique()\n",
    "DEPTS = grid.dept_id.unique()\n",
    "step = DAYS[0]\n",
    "last_day = 1941\n",
    "\n",
    "remove_colums = ['id','item_id','dept_id','cat_id','store_id','state_id','d','sales','wm_yr_wk','date']\n",
    "lags_columns = grid.columns[len(remove_colums):]\n",
    "train_columns = list(grid.columns[~grid.columns.isin(remove_colums)]) + list(lags_columns) \n",
    "print(train_columns)\n",
    "\n",
    "for day in DAYS:\n",
    "    if day!=step:\n",
    "        grid = create_lag_features(grid)\n",
    "\n",
    "    grid[lags_columns] = grid.groupby(['id'], observed=False)[lags_columns].shift(step)\n",
    "    ix_to_drop = grid[(grid['d'] <= 1941) & (grid.isna().any(axis=1))].index\n",
    "    grid.drop(index=ix_to_drop, inplace=True)  \n",
    "      \n",
    "    for store in STORES:\n",
    "        model = lgb.Booster(model_file=f'models/lgbm_{store}.txt')\n",
    "        pred_msk = ( \n",
    "            (grid['d']>last_day+day-step) & \n",
    "            (grid['d']<=last_day+day) & \n",
    "            (grid['store_id']==store)\n",
    "        ) \n",
    "        testX = grid.loc[pred_msk, train_columns]\n",
    "        yhat = model.predict(testX).astype('float16')\n",
    "        tmp_pred = grid.loc[testX.index][['id','d']]\n",
    "        tmp_pred['sales'] = yhat\n",
    "        grid.loc[testX.index, 'sales'] = yhat\n",
    "        predictions = pd.concat([predictions, tmp_pred], axis=0)\n",
    "\n",
    "predictions.to_pickle(f'submissions/recursive.pkl')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "predictions = predictions.pivot(index='id', columns='d', values='sales').reset_index()\n",
    "predictions.columns = submission.columns\n",
    "predictions = submission[['id']].merge(predictions, on='id', how='left').fillna(1)\n",
    "submission_file = \"submissions/submission.csv\"\n",
    "predictions.to_csv(f'{submission_file}', index=False)\n",
    "message = f\"Recursive step: {step}. Start training day: 1700\"\n",
    "competition_name = \"m5-forecasting-accuracy\"\n",
    "submit_to_kaggle(competition_name, submission_file, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  WORKS\n",
    "def update_rolling_mean(grid, day, window_size, shift_size):\n",
    "    # retrieval - the section required to calculate rms\n",
    "    r_end = day - shift_size # 1)1941 2)1955\n",
    "    r_start = r_end - shift_size - window_size\n",
    "    r_mask = (grid['d'] > r_start) & (grid['d'] <= r_end) # 1920-1941\n",
    "    grp = grid[r_mask].groupby('id', group_keys=False, observed=False)['sales']\n",
    "    rolling_mean = grp.transform(lambda x: x.rolling(window_size).mean()).astype('float16')\n",
    "\n",
    "    # values - filtering calculated rms down to the values we are updating\n",
    "    v_end = day - shift_size\n",
    "    v_start = v_end - shift_size\n",
    "    v_mask = (grid['d'] > v_start) & (grid['d'] <= v_end)\n",
    "    rolling_mean = rolling_mean.loc[v_mask]\n",
    "\n",
    "    # update - where the values are going to be placed\n",
    "    u_start = day - shift_size\n",
    "    u_end = day\n",
    "    u_mask = (grid['d'] > u_start) & (grid['d'] <= u_end)\n",
    "    grid.loc[u_mask, f'rm_{window_size}'] = rolling_mean.values\n",
    "    return grid\n",
    "\n",
    "def update_rolling_difference(grid, day, window_size, shift_size):\n",
    "    # retrieval - the section required to calculate rms\n",
    "    # buffer = 200\n",
    "    r_end = day - shift_size # 1941\n",
    "    r_start = r_end - shift_size - window_size #-buffer #Adding an extra d for .diff() computation\n",
    "    r_mask = (grid['d'] > r_start) & (grid['d'] <= r_end) \n",
    "    grp = grid[r_mask].groupby('id', group_keys=False, observed=False)['sales']\n",
    "    rol_diff = grp.apply(lambda x: x.diff().rolling(window=window_size, \n",
    "                                                    min_periods=4).mean()).astype('float16')\n",
    "\n",
    "    # values - filtering calculated rms down to the values we are updating\n",
    "    v_end = day - shift_size\n",
    "    v_start = v_end - shift_size\n",
    "    v_mask = (grid['d'] > v_start) & (grid['d'] <= v_end)\n",
    "    rol_diff = rol_diff.loc[v_mask]\n",
    "\n",
    "    # update - where the values are going to be placed\n",
    "    u_start = day - shift_size\n",
    "    u_end = day\n",
    "    u_mask = (grid['d'] > u_start) & (grid['d'] <= u_end)\n",
    "    grid.loc[u_mask, f'diff_rm_{window_size}'] = rol_diff.values\n",
    "    return grid\n",
    "\n",
    "def update_lags(grid, day, lag_number, shift_size):\n",
    "\n",
    "    r_end = day - shift_size \n",
    "    r_start = r_end - shift_size - lag_number\n",
    "    r_mask = (grid['d'] > r_start) & (grid['d'] <= r_end) # 1920-1941\n",
    "    grp = grid[r_mask].groupby('id', group_keys=False, observed=False)['sales']\n",
    "    lag_values = grp.shift(lag_number)\n",
    "\n",
    "    v_end = day - shift_size\n",
    "    v_start = v_end - shift_size\n",
    "    v_mask = (grid['d'] > v_start) & (grid['d'] <= v_end)\n",
    "    lag_values = lag_values.loc[v_mask]\n",
    "\n",
    "    u_start = day - shift_size\n",
    "    u_end = day\n",
    "    u_mask = (grid['d'] > u_start) & (grid['d'] <= u_end)\n",
    "    grid.loc[u_mask, f'lag_{lag_number}'] = lag_values.values\n",
    "    return grid\n",
    "\n",
    "def update_rolling_stats(grid, day, window_size, shift_size):\n",
    "\n",
    "    r_end = day - shift_size # 1)1941 2)1955\n",
    "    r_start = r_end - shift_size - window_size\n",
    "    r_mask = (grid['d'] > r_start) & (grid['d'] <= r_end) # 1920-1941\n",
    "    grp = grid[r_mask].groupby('id', group_keys=False, observed=False)['sales']\n",
    "    rol_std = grp.transform(lambda x: x.rolling(window_size).std()).astype('float16')\n",
    "    rol_max = grp.transform(lambda x: x.rolling(window_size).max()).astype('float16')\n",
    "\n",
    "    # values - filtering calculated rms down to the values we are updating\n",
    "    v_end = day - shift_size\n",
    "    v_start = v_end - shift_size\n",
    "    v_mask = (grid['d'] > v_start) & (grid['d'] <= v_end)\n",
    "    rol_std = rol_std.loc[v_mask]\n",
    "    rol_max = rol_max.loc[v_mask]\n",
    "\n",
    "    u_start = day - shift_size\n",
    "    u_end = day\n",
    "    u_mask = (grid['d'] > u_start) & (grid['d'] <= u_end)\n",
    "    grid.loc[u_mask, f'std_{window_size}'] = rol_std.values\n",
    "    grid.loc[u_mask, f'max_{window_size}'] = rol_max.values\n",
    "    return grid\n",
    "\n",
    "def update_rolling_zeroes(grid, day, window_size, shift_size):\n",
    "\n",
    "    min_periods = int(window_size/3)\n",
    "    r_end = day - shift_size # 1)1941 2)1955\n",
    "    r_start = r_end - shift_size - window_size\n",
    "    r_mask = (grid['d'] > r_start) & (grid['d'] <= r_end) # 1920-1941\n",
    "\n",
    "    temp_zero = grid.loc[r_mask][['id','d','sales']]\n",
    "    temp_zero['is_zero'] = [1 if sales == 0 else 0 for sales in temp_zero['sales']]\n",
    "    grp = temp_zero.groupby(['id'], group_keys=False, observed=False)['sales']\n",
    "    rol_zero = grp.transform(lambda x: x.rolling(window=window_size,\n",
    "                                                 min_periods=min_periods).sum()).astype('float16')\n",
    "    \n",
    "    v_end = day - shift_size\n",
    "    v_start = v_end - shift_size\n",
    "    v_mask = (grid['d'] > v_start) & (grid['d'] <= v_end)\n",
    "    rol_zero = rol_zero.loc[v_mask]    \n",
    "    \n",
    "    u_start = day - shift_size\n",
    "    u_end = day\n",
    "    u_mask = (grid['d'] > u_start) & (grid['d'] <= u_end)\n",
    "    grid.loc[u_mask, f'rolling_zero_{window_size}'] = rol_zero.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
