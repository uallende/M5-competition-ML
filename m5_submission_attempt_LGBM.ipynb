{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print(get_df_name(df), ' Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'subsample': 0.5,\n",
    "        'device': 'gpu',\n",
    "        'subsample_freq': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 2 ** 11 - 1,\n",
    "        'min_data_in_leaf': 2 ** 12 - 1,\n",
    "        'feature_fraction': 0.5,\n",
    "        'max_bin': 100,\n",
    "        'n_estimators': 1400,\n",
    "        'boost_from_average': False,\n",
    "        'verbosity': -1\n",
    "        }\n",
    "\n",
    "raw_data = pd.read_csv(r'data\\sales_train_evaluation.csv')\n",
    "calendar = pd.read_csv(r'data\\calendar.csv')\n",
    "sell_prices = pd.read_csv(r'data\\sell_prices.csv')\n",
    "submission = pd.read_csv(r'data\\sample_submission.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.read_pickle(r'df_nofeat_noscale.pkl')\n",
    "grid.date = grid.date.astype('datetime64[ns]')\n",
    "\n",
    "grid['tm_d'] = grid['date'].dt.day.astype(np.int8)\n",
    "grid['tm_w'] = grid['date'].dt.week.astype(np.int8)\n",
    "grid['tm_m'] = grid['date'].dt.month.astype(np.int8)\n",
    "grid['tm_y'] = grid['date'].dt.year\n",
    "grid['tm_y'] = (grid['tm_y'] - grid['tm_y'].min()).astype(np.int8)\n",
    "grid['tm_dw'] = grid['date'].dt.dayofweek.astype(np.int8)\n",
    "grid['tm_w_end'] = (grid['tm_dw'] >= 5).astype(np.int8)\n",
    "\n",
    "horizon = 0\n",
    "print('************ ROLLING MEANS ************')\n",
    "grp = grid.groupby(['id'], group_keys=False)['sales']\n",
    "for roll in [7,14,30,60,180]:\n",
    "    grid['rm_' + str(roll)] = grp.apply(lambda x: x.shift(horizon).rolling(roll).mean())\n",
    "    \n",
    "print('************ ROLLING STATS ************')\n",
    "for roll in [7,14,30,60,180]:\n",
    "    grid['max_' + str(roll)] = grp.apply(lambda x: x.shift(horizon).rolling(roll).max())\n",
    "#     grid['min_' + str(roll)] = grp.apply(lambda x: x.shift(horizon).rolling(roll).min())\n",
    "    grid['std_' + str(roll)] = grp.apply(lambda x: x.shift(horizon).rolling(roll).std())\n",
    "#     grid['median_' + str(roll)] = grp.apply(lambda x: x.shift(horizon).rolling(roll).median())\n",
    "\n",
    "print('************ DIFF MEANS ************')\n",
    "for l in [7,56,140]:\n",
    "    grid['diff_rm_' + str(l)] = grp.apply(lambda x : x.shift(horizon).diff().rolling(l).mean())\n",
    "\n",
    "grp = grid.groupby(['id'], group_keys=False)['sales']\n",
    "print('************ LAGS ************')\n",
    "for lag in [1,2,3,4,5,6,7]:\n",
    "    grid['lag_' + str(lag)] = grp.apply(lambda x: x.shift(lag))\n",
    "    \n",
    "print('************ ROLLING ZEROS ************')\n",
    "for roll in [7,56,140]:\n",
    "    grid['is_zero'] = [1 if sales == 0 else 0 for sales in grid['sales']]\n",
    "    grp = grid.groupby(['id'], group_keys=False)['is_zero']\n",
    "    grid['rolling_zero_' + str(roll)] = grp.apply(lambda x : x.shift(horizon).rolling(roll).sum())\n",
    "    grid = grid.drop('is_zero', axis = 1)   \n",
    "   \n",
    "print('************ ROLLING PRICES ************')\n",
    "# # # We can do some basic aggregations\n",
    "grp = grid.groupby(['id'], group_keys=False)['sell_price']\n",
    "grid['price_max'] = grp.apply(lambda x : x.expanding().max())\n",
    "grid['price_min'] = grp.apply(lambda x : x.expanding().min())\n",
    "grid['price_std'] = grp.apply(lambda x : x.expanding().std())\n",
    "grid['price_mean'] = grp.apply(lambda x : x.expanding().mean())\n",
    "# and do price normalization (min/max scaling)\n",
    "grid['price_norm'] = grid['sell_price']/grid['price_max']\n",
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "grid['price_nunique'] = grid.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "grid['item_nunique'] = grid.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "# last two years of data\n",
    "grid = grid.dropna()\n",
    "grid = reduce_mem_usage(grid)\n",
    "grid.shape\n",
    "grid.to_pickle(r'data\\df_feat_no_shift_2.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stores & Dept models - Multi-step model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = [4,8,12,16,20,24,28] # Training/Prediction every 4 days (Compromise between very granular and long time horizon)\n",
    "VAL_DAYS, TEST_DAYS = STEPS[0], STEPS[0]\n",
    "STORES = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "TARGET = ['sales']\n",
    "\n",
    "horizon = 28\n",
    "first_train_day = 1850\n",
    "last_train_day = 1941 - horizon\n",
    "\n",
    "first_val_day = last_train_day + 1\n",
    "last_val_day = 1941\n",
    "\n",
    "first_pred_day = 1941 + 1\n",
    "train_days = last_train_day-first_train_day\n",
    "\n",
    "train_start = first_train_day\n",
    "train_end = last_train_day\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "remove_colums = ['item_id', 'store_id', 'state_id', 'id', 'd', 'date', 'weekday',\n",
    "       'sales', 'year']\n",
    "\n",
    "grid = pd.read_pickle(r'data\\df_feat_no_shift_2.pkl')\n",
    "train_columns = grid.columns[~grid.columns.isin(remove_colums)]\n",
    "\n",
    "DEPTS = grid.dept_id.unique()\n",
    "\n",
    "lags_columns = ['rm_7', 'rm_14', 'rm_30', 'rm_60', 'rm_180', 'max_7', 'std_7', 'max_14', 'std_14', 'max_30', \n",
    "       'std_30', 'max_60', 'std_60', 'max_180', 'std_180', 'diff_rm_7', 'diff_rm_56', 'diff_rm_140', 'lag_1',\n",
    "       'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_zero_7', 'rolling_zero_56', 'rolling_zero_140']\n",
    "\n",
    "for store in STORES:\n",
    "\n",
    "       for dept in DEPTS:\n",
    "\n",
    "              for step in STEPS:\n",
    "\n",
    "                     grid = pd.read_pickle(r'data\\Projects\\df_feat_no_shift_2.pkl')\n",
    "                     grid[['dept_id', 'store_id', 'id']] = grid[['dept_id', 'store_id', 'id']].astype('category')\n",
    "                     grid = grid[(grid['store_id'] == store) & (grid['dept_id'] == dept)]\n",
    "                     grid[lags_columns] = grid.groupby(['id'])[lags_columns].shift(step)\n",
    "                     grid = grid.dropna()\n",
    "\n",
    "                     val_start = first_val_day + step - VAL_DAYS\n",
    "                     val_end = first_val_day + step - 1\n",
    "                     pred_start = first_pred_day + step - VAL_DAYS \n",
    "                     pred_end = first_pred_day + step - 1\n",
    "\n",
    "                     #print('pred_start: ', pred_start, 'pred_end: ', pred_end, 'val_start: ', val_start, 'val_end: ', val_end)\n",
    "\n",
    "                     trainX = grid[(grid['d'] <= train_end)][train_columns]\n",
    "                     trainY = grid[(grid['d'] <= train_end)][TARGET]\n",
    "\n",
    "                     valX = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][train_columns]\n",
    "                     valY = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][TARGET]\n",
    "\n",
    "                     testX = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][train_columns]\n",
    "\n",
    "                     lgb_params = {\n",
    "                            'boosting_type': 'gbdt',\n",
    "                            'objective': 'tweedie',\n",
    "                            'tweedie_variance_power': 1.1,\n",
    "                            'metric': 'rmse',\n",
    "                            'subsample': 0.5,\n",
    "                            'device': 'gpu',\n",
    "                            'subsample_freq': 1,\n",
    "                            'learning_rate': 0.03,\n",
    "                            'num_leaves': 2 ** 11 - 1,\n",
    "                            'min_data_in_leaf': 2 ** 12 - 1,\n",
    "                            'feature_fraction': 0.5,\n",
    "                            'max_bin': 100,\n",
    "                            'n_estimators': 1400,\n",
    "                            'boost_from_average': False,\n",
    "                            'verbose': -1\n",
    "                            }\n",
    "\n",
    "                     # Train\n",
    "                     lgbm = LGBMRegressor(**lgb_params)\n",
    "                     lgbm.fit(trainX, trainY, eval_set=[(valX, valY)], eval_metric='rmse', early_stopping_rounds=50)\n",
    "\n",
    "                     # Predict\n",
    "                     yhat = lgbm.predict(testX, num_iteration=lgbm.best_iteration_)\n",
    "                     preds = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][['id', 'd']]\n",
    "                     preds['sales'] = yhat\n",
    "                     predictions = pd.concat([predictions, preds], axis=0)\n",
    "\n",
    "predictions.to_pickle('sub_7_models_alltrainingdata_dept&stores.pkl')\n",
    "# Submission\n",
    "submission = pd.read_csv(r'\\data\\sample_submission.csv')\n",
    "predictions = predictions.pivot(index='id', columns='d', values='sales').reset_index()\n",
    "predictions.columns = submission.columns\n",
    "predictions = submission[['id']].merge(predictions, on='id', how='left').fillna(1)\n",
    "predictions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stores Model - Multi-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = [4,8,12,16,20,24,28]\n",
    "VAL_DAYS, TEST_DAYS = STEPS[0], STEPS[0]\n",
    "STORES = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "TARGET = ['sales']\n",
    "\n",
    "horizon = 28\n",
    "first_train_day = 1850\n",
    "last_train_day = 1941 - horizon\n",
    "\n",
    "first_val_day = last_train_day + 1\n",
    "last_val_day = 1941\n",
    "\n",
    "first_pred_day = 1941 + 1\n",
    "train_days = last_train_day-first_train_day\n",
    "\n",
    "train_start = first_train_day\n",
    "train_end = last_train_day\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "remove_colums = ['item_id', 'store_id', 'state_id', 'id', 'd', 'date', 'weekday',\n",
    "       'sales', 'year']\n",
    "\n",
    "grid = pd.read_pickle(r'data\\df_feat_no_shift_2.pkl')\n",
    "train_columns = grid.columns[~grid.columns.isin(remove_colums)]\n",
    "\n",
    "DEPTS = grid.dept_id.unique()\n",
    "\n",
    "lags_columns = ['rm_7', 'rm_14', 'rm_30', 'rm_60', 'rm_180', 'max_7', 'std_7', 'max_14', 'std_14', 'max_30', \n",
    "       'std_30', 'max_60', 'std_60', 'max_180', 'std_180', 'diff_rm_7', 'diff_rm_56', 'diff_rm_140', 'lag_1',\n",
    "       'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_zero_7', 'rolling_zero_56', 'rolling_zero_140']\n",
    "\n",
    "for store in STORES:\n",
    "\n",
    "       for step in STEPS:\n",
    "\n",
    "              grid = pd.read_pickle(r'data\\df_feat_no_shift_2.pkl')\n",
    "              grid[['dept_id', 'store_id', 'id']] = grid[['dept_id', 'store_id', 'id']].astype('category')\n",
    "              grid = grid[(grid['store_id'] == store) & (grid['dept_id'] == dept)]\n",
    "              grid[lags_columns] = grid.groupby(['id'])[lags_columns].shift(step)\n",
    "              grid = grid.dropna()\n",
    "\n",
    "              val_start = first_val_day + step - VAL_DAYS\n",
    "              val_end = first_val_day + step - 1\n",
    "              pred_start = first_pred_day + step - VAL_DAYS \n",
    "              pred_end = first_pred_day + step - 1\n",
    "\n",
    "              trainX = grid[(grid['d'] <= train_end)][train_columns]\n",
    "              trainY = grid[(grid['d'] <= train_end)][TARGET]\n",
    "\n",
    "              valX = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][train_columns]\n",
    "              valY = grid[(grid['d'] >= val_start) & (grid['d'] <= val_end)][TARGET]\n",
    "\n",
    "              testX = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][train_columns]\n",
    "\n",
    "              lgb_params = {\n",
    "                     'boosting_type': 'gbdt',\n",
    "                     'objective': 'tweedie',\n",
    "                     'tweedie_variance_power': 1.1,\n",
    "                     'metric': 'rmse',\n",
    "                     'subsample': 0.5,\n",
    "                     'device': 'gpu',\n",
    "                     'subsample_freq': 1,\n",
    "                     'learning_rate': 0.03,\n",
    "                     'num_leaves': 2 ** 11 - 1,\n",
    "                     'min_data_in_leaf': 2 ** 12 - 1,\n",
    "                     'feature_fraction': 0.5,\n",
    "                     'max_bin': 100,\n",
    "                     'n_estimators': 1400,\n",
    "                     'boost_from_average': False,\n",
    "                     'verbose': -1\n",
    "                     }\n",
    "\n",
    "              # Train\n",
    "              lgbm = LGBMRegressor(**lgb_params)\n",
    "              lgbm.fit(trainX, trainY, eval_set=[(valX, valY)], eval_metric='rmse', early_stopping_rounds=50)\n",
    "\n",
    "              # Predictions\n",
    "              yhat = lgbm.predict(testX, num_iteration=lgbm.best_iteration_)\n",
    "              preds = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][['id', 'd']]\n",
    "              preds['sales'] = yhat\n",
    "              predictions = pd.concat([predictions, preds], axis=0)\n",
    "\n",
    "predictions.to_pickle('sub_14_models_alltrainingdata_stores.pkl')\n",
    "# Submission\n",
    "submission = pd.read_csv(r'\\data\\sample_submission.csv')\n",
    "predictions = predictions.pivot(index='id', columns='d', values='sales').reset_index()\n",
    "predictions.columns = submission.columns\n",
    "predictions = submission[['id']].merge(predictions, on='id', how='left').fillna(1)\n",
    "predictions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1 = pd.read_pickle('store_dept_14_models.pkl')\n",
    "# pred2 = pd.read_pickle('global_14_models.pkl')\n",
    "# pred3 = pd.read_pickle('dept_14_models.pkl')\n",
    "# pred4 = pd.read_pickle('stores_14_models.pkl')\n",
    "\n",
    "# pred1 = pd.read_pickle('store_dept_4_models.pkl')\n",
    "# pred2 = pd.read_pickle('global_4_models.pkl')\n",
    "# pred3 = pd.read_pickle('dept_4_models.pkl')\n",
    "# pred4 = pd.read_pickle('stores_4_models.pkl')\n",
    "\n",
    "#pred1 = pd.read_pickle('sub_4_models_alltrainingdata_dept&stores.pkl')\n",
    "#pred2 = pd.read_pickle('sub_4_models_alltrainingdata_stores.pkl')\n",
    "#pred3 = pd.read_pickle('store_dept_14_models.pkl')\n",
    "#pred4 = pd.read_pickle('stores_14_models.pkl')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "submission = pd.read_csv(r'\\data\\sample_submission.csv')\n",
    "#results = pd.concat([pred1, pred2, pred3, pred4], axis=0)\n",
    "results = pd.concat([pred1, pred3], axis=0)\n",
    "results = results.groupby(['id', 'd']).mean().reset_index()\n",
    "results = results.pivot(index='id', columns='d', values='sales').reset_index()\n",
    "results.columns = submission.columns\n",
    "results = submission[['id']].merge(results, on='id', how='left').fillna(1)\n",
    "results.to_csv('submission_4.csv', index=False)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60c112127e11c5371fc1371160fc4d27b7aede14e0d229d3bdb847bc9c3e674c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
